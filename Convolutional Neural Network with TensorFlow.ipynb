{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks with TensorFlow\n",
    "===\n",
    "**Most cutting edge machine learning technology**\n",
    "\n",
    "\"Not sure we can make an improvement on 97.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "# Image Tools\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL        import Image\n",
    "\n",
    "# filesystem tools that allow for file manipulation\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Machine Learning Tools\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist  = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this image set had been flattened. Convolutional Neural Networks want 2D inputs.\n",
    "- They walk the convolution kernel over the entire image\n",
    "\n",
    "**We need to reshape our original data set from flattened into 'real' images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.reshape(-1, 28,28,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights from Below:\n",
    "\n",
    "1. It seems that the **big** difference between a 'canonical' neural network and a 'convolutional' neural network\n",
    "is that in the forward propogation, or prediction, function (here call 'model'), a canonical NN uses `tf.matmul` or matrix multiplication (a.k.a. the dot product) to 'mix' the weights with the layers.  **But**, the 'convolutional' NN uses a 2D convolution to 'mix' the weights with the layers.\n",
    "    - this is a **big**, **big** deal; it's a major difference between the two methods and also creates a completely different layout for the interpretation of how the network interacts with the data, layers, and predictions.\n",
    " \n",
    "2. Softmax creates a logit layer(?)\n",
    "    - Mike Bernico said \"and this will return to logit layer\" when doing the `tf.nn.softmax` operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Configure and Generate the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Variables: Hyper Parameters\n",
    "        batch_size    = 128   # Mini batch size for SGD (Stochastic Gradient Descent)\n",
    "        num_hidden    = 1024  # Depth of the hidden layer\n",
    "        keep_prob     = 0.5   # Dropout keep probability (50%)\n",
    "        num_labels    = 10    # One hot encoding ==> 10 labels; one for each of the 10 classes (0,1)\n",
    "        patch_size    = 5     # THIS IS NEW\n",
    "        num_channels  = 1     # Grayscale:1, RGB:1 THIS IS NEW\n",
    "        depth         = 16    # THIS IS NEW\n",
    "        image_size    = 28\n",
    "        \n",
    "        def weight_variable(shape):\n",
    "            # There are going to be so many weight variables, that is it frugle\n",
    "            #   to create a function that returns the initialized tensors for you\n",
    "            # Function takes in `shape` and returns `tf.Variable` with \n",
    "            #  `tf.truncated_normal` config\n",
    "            stddev0 = 0.1\n",
    "            initial = tf.truncated_normal(shape, stddev=stddev0)\n",
    "            return tf.Variable(initial)\n",
    "        \n",
    "        def bias_variable(shape):\n",
    "            # There are going to be so many bias variables, that is it frugle\n",
    "            #   to create a function that returns the initialized tensors for you\n",
    "            # Function takes in `shape` and returns `tf.Variable` with `tf.constant` config\n",
    "            b0      = 0.1\n",
    "            initial = tf.constant(b0, shape=shape) ## ** Why is this `tf.constant` instead of `tf.zeros`\n",
    "            return tf.Variable(initial)\n",
    "        \n",
    "        def conv2d(x,W):\n",
    "            # Build 2D Convolutional layers for the network\n",
    "            # We are going to need so many of them, it's better to adapt \n",
    "            #   a function to initialize them\n",
    "            strides0  = [1,1,1,1]\n",
    "            return tf.nn.conv2d(x, W, strides=strides0, padding='SAME')\n",
    "        \n",
    "        def max_pool_2x2(x):\n",
    "            # Build 2D Max Pooling layers for the network\n",
    "            # We are going to need so many of them, it's better to adapt \n",
    "            #   a function to initialize them\n",
    "            ksize0  = [1,2,2,1]\n",
    "            strides1= [1,2,2,1]\n",
    "            return tf.nn.max_pool(x, ksize=ksize0, strides=strides1, padding='SAME')\n",
    "        \n",
    "        # Input Data. For the training data, we use a placeholder that will be fed\n",
    "        #   at run time with a training minibatch.\n",
    "        #\n",
    "        # *** GETTING THE MATRICES RIGHT IS PROBABLY THE HARDEST PART ABOUT USING ConvNets\n",
    "        tf_train_dataset  = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels   = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        \n",
    "        # Have to reshape the data set to be the correct size for 2D images in the 2D convolutions\n",
    "        tf_valid_dataset  = tf.constant(mnist.validation.images.reshape(-1, 28,28,1))\n",
    "        tf_test_dataset   = tf.constant(mnist.test.images.reshape(-1, 28,28,1))\n",
    "        \n",
    "        keep_prob         = tf.placeholder(tf.float32) # last time it was `tf.placeholder('float')`\n",
    "        \n",
    "        ## Initialize the Variables for all of the Weights and Biases\n",
    "        \n",
    "        # Two Convolutionaly (\"conv\") layers: 2 weights, 2 biases\n",
    "        W_conv1 = weight_variable([5,5,1,32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        W_conv2 = weight_variable([5,5,32,64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        \n",
    "        # Two fully connect (\"fc\") layers (max pooling?): 2 weights, 2 biases\n",
    "        W_fc1   = weight_variable([7*7*64, 1024])\n",
    "        b_fc1   = bias_variable([1024])\n",
    "        W_fc2   = weight_variable([1024, 10])\n",
    "        b_fc2   = bias_variable([10])\n",
    "        \n",
    "        def model(data):\n",
    "            \"\"\" Assembles the NN \"\"\"\n",
    "            h_conv1 = tf.nn.relu(conv2d(data, W_conv1) + b_conv1)\n",
    "            h_pool1 = max_pool_2x2(h_conv1)\n",
    "            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "            h_pool2 = max_pool_2x2(h_conv2)\n",
    "            \n",
    "            # We have to flatten the weights in order to 'mix' them with the more \n",
    "            #   'canonical', fully connected layers\n",
    "            h_pool2_flat  = tf.reshape(h_pool2, [-1, 7*7*64]) \n",
    "            \n",
    "            # Also acts like a hidden layer with ReLU activation\n",
    "            h_fc1   = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "            \n",
    "            # Now process that through the dropout 'layer' or 'filter'\n",
    "            h_fc1_drop  = tf.nn.dropout(h_fc1, keep_prob)\n",
    "            \n",
    "            # Logit layer, return the softmax of the matrix multiplication\n",
    "            #  ** GUESS ** softmax == logit layer?\n",
    "            return tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "        \n",
    "        # Training Computations\n",
    "        logits  = model(tf_train_dataset) # prediction from `tf_train_dataset` placeholder\n",
    "        loss    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "        ### ****************************************** ###\n",
    "        ### Why is there no regularized_loss term?\n",
    "        ### Instructor \"removed the regularization because it just gets too complicated with this many layers\n",
    "        ### ****************************************** ###\n",
    "        \n",
    "        # Optimizer: Stochastic Gradient Descent (SGD)\n",
    "        SGD_step_size = 0.5\n",
    "        optimizer     = tf.train.GradientDescentOptimizer(SGD_step_size).minimize(loss)\n",
    "        \n",
    "        ### ******************************************************************************** ###\n",
    "        ### I can't believe that I forgot the ACTIVATE the minimize with `.minimize(loss)`   ###\n",
    "        ### ******************************************************************************** ###\n",
    "        \n",
    "        # Predictions for the training, validation, and test data\n",
    "        train_prediction  = tf.nn.softmax(logits) # WHY are we doing a SOFTMAX on the SOFTMAX?\n",
    "        valid_prediction  = tf.nn.softmax(model(tf_valid_dataset))\n",
    "        test_prediction   = tf.nn.softmax(model(tf_test_dataset))\n",
    "        \n",
    "        # Initialize Saver\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0*np.sum(np.argmax(predictions, 1) == np.argmax(labels,1)) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jonathan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized at 1511385435.79\n",
      "Minibatch loss at step 0: 2.348978\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 9.9%\n",
      "This operation took 0.34 seconds\n",
      "Minibatch loss at step 100: 2.343959\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 20.89 seconds\n",
      "Minibatch loss at step 200: 2.359588\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 22.92 seconds\n",
      "Minibatch loss at step 300: 2.359588\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 23.21 seconds\n",
      "Minibatch loss at step 400: 2.343963\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 23.49 seconds\n",
      "Minibatch loss at step 500: 2.359588\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 23.38 seconds\n",
      "Minibatch loss at step 600: 2.336151\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.0%\n",
      "This operation took 23.95 seconds\n",
      "Minibatch loss at step 700: 2.105695\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 35.5%\n",
      "This operation took 23.38 seconds\n",
      "Minibatch loss at step 800: 1.940828\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 57.5%\n",
      "This operation took 25.74 seconds\n",
      "Minibatch loss at step 900: 1.813283\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 75.3%\n",
      "This operation took 25.87 seconds\n",
      "Minibatch loss at step 1000: 1.665668\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.5%\n",
      "This operation took 24.34 seconds\n",
      "Minibatch loss at step 1100: 1.571164\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.2%\n",
      "This operation took 21.83 seconds\n",
      "Minibatch loss at step 1200: 1.585286\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "This operation took 21.85 seconds\n",
      "Minibatch loss at step 1300: 1.540033\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.6%\n",
      "This operation took 22.05 seconds\n",
      "Minibatch loss at step 1400: 1.571820\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.7%\n",
      "This operation took 21.71 seconds\n",
      "Minibatch loss at step 1500: 1.566930\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.1%\n",
      "This operation took 21.67 seconds\n",
      "Minibatch loss at step 1600: 1.540097\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.0%\n",
      "This operation took 21.75 seconds\n",
      "Minibatch loss at step 1700: 1.592978\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.2%\n",
      "This operation took 21.73 seconds\n",
      "Minibatch loss at step 1800: 1.533607\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.4%\n",
      "This operation took 21.69 seconds\n",
      "Minibatch loss at step 1900: 1.585298\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "This operation took 21.62 seconds\n",
      "Model saved in file: ./conv_net_mnist_model.ckpt\n",
      "Test accuracy: 88.2%\n",
      "This operation took 523.54 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "num_steps     = 2000\n",
    "print_step    = 100\n",
    "time_add      = 0\n",
    "\n",
    "on_keep_prob  = 0.5\n",
    "off_keep_prob = 1.0\n",
    "\n",
    "start0 = time()\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized at %.2f\" % time())\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        start1 = time()\n",
    "        # Generate a minibatch\n",
    "        batch_data, batch_labels  = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Reshape this batch\n",
    "        batch_data  = batch_data.reshape(-1,28,28,1)\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch\n",
    "        # The `keys` for the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the `values` are the numpy array to feed to it\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : on_keep_prob}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        time_add += (time() - start1)\n",
    "        if step % print_step == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            valid_predNow = valid_prediction.eval(feed_dict={keep_prob:off_keep_prob})\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_predNow, mnist.validation.labels.astype(float)))\n",
    "            print(\"This operation took %.2f seconds\" % time_add)\n",
    "            time_add  = 0\n",
    "    \n",
    "    save_path = saver.save(session, \"./conv_net_mnist_model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    test_predNow = test_prediction.eval(feed_dict={keep_prob:off_keep_prob})\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_predNow, mnist.test.labels.astype(float)))\n",
    "    print(\"This operation took %.2f seconds\" % (time() - start0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks Results\n",
    "\n",
    "**BLOWN AWAY Part 1:** Mike Bernico's code achieved **95.9%** Validation Accuracy in just **500** steps!!\n",
    "\n",
    "**BLOWN AWAY Part 2:** Mike Bernico's code achieved **98.9%** Validation Accuracy after 8000 steps!!\n",
    "\n",
    "**BLOWN AWAY Part 3:** Mike Bernico's code achieved **99.0%** Test Accuracy after 8000 steps (and 10 minutes)!!\n",
    "\n",
    "1. OMG this is much much slower; but that was definitely expected!\n",
    "2. My accuracy step takes longer than if 500 iterations\n",
    "    - I must not be using multiprocessing\n",
    "    - Just to compute the validation accuracy took about 5 minutes D:\n",
    "3. Instructor says \"the world record for MNIST is 99.77% accuracy\"\n",
    "    - He thinks that 99.0% is not close enough to be excited about, but I am blown away!\n",
    "    - Maybe some hyperparameter manipulation could do it\n",
    "4. World record was set using a ConvNN like this one.\n",
    "    - They didn't use ONE, they used an ensemble of 34 convolutional NN's\n",
    "    - Just like DROPOUT, more than one is always better than just one\n",
    "    - **Ensembles are a way to usually improve a model**\n",
    "5. ConvNN are really good at traditional (trivial) problems like MNIST, but they **revolutionary** at real image recognition\n",
    "    - They can do much more amazing things when you want to recognize is it a cat or a dog\n",
    "        - is it a broken car or a fixed car\n",
    "        - is it a person in the cross walk or a light\n",
    "    - **All** of the self-driving car technology revolve around what we've done with ConvNets\n",
    "6. History of this lesson:\n",
    "    - Started with multinomial logistic regression\n",
    "    - Created a multilayer perceptron to solve the MLR (above)\n",
    "    - Now we've implemented the convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
